{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac0438f2-b407-41c6-bde6-a14ef03e67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import PartialDependenceDisplay, partial_dependence\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import DBSCAN\n",
    "import time\n",
    "import matplotlib.pylab as pylab\n",
    "# params = {'legend.fontsize':12,\n",
    "#           'axes.labelsize':16,\n",
    "#          'axes.titlesize':16,\n",
    "#          'xtick.labelsize':12,\n",
    "#          'ytick.labelsize':12}\n",
    "# pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2727e816-0ca5-40f8-9051-d029ebcd5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].notna().all():\n",
    "            if col != 'Wind_Speed':\n",
    "                df[col] = df[col].astype(int)\n",
    "            else:\n",
    "                df[col] = round(df[col], 1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86da3787-eed1-4fda-98f8-d4373bffb751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_round(x, base):\n",
    "    \n",
    "    return int(base * round(float(x)/base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b480cfb-364e-4b00-b064-36231e86b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    feature_1 = 'Wind_Speed'\n",
    "    feature_2 = 'Prod_Pwr'\n",
    "    X = df.loc[:'2016'][[feature_1, feature_2]].values\n",
    "    db = DBSCAN(eps=1.4, min_samples=10).fit(X)\n",
    "    labels = db.labels_\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22993313-a67c-4c02-82da-7bb1e90ec168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and clean the data. Requires csv files to be located in a folder called \"CSV\" in the directory of the notebook.\n",
    "def process_data():     \n",
    "    # Load the data\n",
    "    df_signals_2016 = pd.read_csv('CSV\\wind-farm-signals-2016.csv', sep=';')\n",
    "    df_signals_2017 = pd.read_csv('CSV\\wind-farm-signals-2017_bruk.csv', sep=';')\n",
    "    df_metmast_2016 = pd.read_csv('CSV\\wind-farm-metmast-2016.csv', sep=';')\n",
    "    df_metmast_2017 = pd.read_csv('CSV\\wind-farm-metmast-2017.csv', sep=';')\n",
    "    \n",
    "    # Merge dataframes\n",
    "    frames1 = [df_signals_2016, df_signals_2017]\n",
    "    df1 = pd.concat(frames1, ignore_index=True)\n",
    "    # Convert to datetime and remove time zone\n",
    "    df1['Timestamp'] = pd.to_datetime(df1['Timestamp'], dayfirst=True).dt.tz_localize(None)\n",
    "\n",
    "    frames2 = [df_metmast_2016, df_metmast_2017]\n",
    "    df2 = pd.concat(frames2, ignore_index=True)\n",
    "    df2['Timestamp'] = pd.to_datetime(df2['Timestamp'], dayfirst=True).dt.tz_localize(None)\n",
    "\n",
    "    # Join dataframes on index 'Timestamp'\n",
    "    df = df1.set_index('Timestamp').join(df2.set_index('Timestamp'))\n",
    "\n",
    "    # Select turbine\n",
    "    turbine = 'T07'\n",
    "    df = df.loc[df['Turbine_ID'] == turbine].reset_index()\n",
    "\n",
    "    # Extract only relevant columns\n",
    "    df = df[['Timestamp', 'Turbine_ID', 'Gen_Bear_Temp_Avg', 'Gen_Bear2_Temp_Avg',\n",
    "             'Gen_RPM_Avg', 'Nac_Temp_Avg', 'Amb_WindSpeed_Avg', 'Avg_Humidity', \n",
    "             'Gen_Phase1_Temp_Avg', 'Gen_Phase2_Temp_Avg', 'Gen_Phase3_Temp_Avg',\n",
    "             'Amb_Temp_Avg', 'Grd_Prod_Pwr_Avg',\n",
    "             'Amb_WindDir_Abs_Avg',\n",
    "             'Rtr_RPM_Avg'\n",
    "            ]].copy()\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={'Timestamp': 'Date',\n",
    "                            'Gen_Bear_Temp_Avg': 'Gen_Bear_Temp',\n",
    "                            'Gen_RPM_Avg': 'Gen_RPM',\n",
    "                            'Gen_Bear2_Temp_Avg': 'Gen_Bear2_Temp',\n",
    "                            'Nac_Temp_Avg': 'Nac_Temp', \n",
    "                            'Amb_WindSpeed_Avg': 'Wind_Speed', \n",
    "                            'Avg_Humidity': 'Humidity',\n",
    "                            'Gen_Phase1_Temp_Avg': 'Gen_Phase1_Temp', \n",
    "                            'Gen_Phase2_Temp_Avg': 'Gen_Phase2_Temp',\n",
    "                            'Gen_Phase3_Temp_Avg': 'Gen_Phase3_Temp',\n",
    "                            'Grd_Prod_Pwr_Avg': 'Prod_Pwr',\n",
    "                            'Amb_WindDir_Abs_Avg': 'Wind_Dir',\n",
    "                            'Amb_Temp_Avg': 'Amb_Temp',\n",
    "                            'Rtr_RPM_Avg': 'Rtr_RPM',\n",
    "                            'Grd_Prod_Pwr_Avg': 'Prod_Pwr'})\n",
    "\n",
    "    df['Gen_Phase_Temp'] = df[['Gen_Phase1_Temp', 'Gen_Phase2_Temp', 'Gen_Phase3_Temp']].mean(axis=1)\n",
    "    df = df.drop(columns=['Gen_Phase1_Temp', 'Gen_Phase2_Temp', 'Gen_Phase3_Temp'])\n",
    "\n",
    "    # Fill the missing Gen_Bear_Temp nan value with the the mean of the values diorectly next to it\n",
    "    df['Gen_Bear_Temp'] = df['Gen_Bear_Temp'].fillna(48)\n",
    "    df['Humidity'] = df['Humidity'].interpolate(method='linear').round()\n",
    "#     df = df.loc[df['Gen_Bear_Temp'] < 100]\n",
    "\n",
    "    # Combine duplicates by their mean\n",
    "    df = df.groupby(df['Date']).mean(numeric_only=True).reset_index()\n",
    "    df = df.set_index('Date')\n",
    "\n",
    "    df = df.round(1)\n",
    "\n",
    "    df = df.query('~(Gen_Bear_Temp > 100 & index < 2017)')\n",
    "    df = df.query('~(Prod_Pwr < 0 & Wind_Speed >= 4 & index < 2017)')\n",
    "    \n",
    "    labels = remove_outliers(df)\n",
    "    labels = pd.Series(labels, df.loc[:'2016'].index, name='Label')\n",
    "    df = pd.concat([df, labels], axis=1) \n",
    "    df['Label'] = df['Label'].fillna(0)\n",
    "    df['Label'] = df['Label'].astype('int')\n",
    "    df = df.query('Label == 0')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ef06be73-714f-435f-b05e-2732243e6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "def train_models(X_train, y_train):\n",
    "\n",
    "#     start = time.time()\n",
    "#     LR = LinearRegression(n_jobs=-1)\n",
    "#     LR.fit(X_train, y_train)\n",
    "#     end = time.time()\n",
    "#     print(end - start)\n",
    "    \n",
    "#     start = time.time()\n",
    "#     RF = RandomForestRegressor(random_state=42)\n",
    "#     RF.fit(X_train, y_train)\n",
    "#     end = time.time()\n",
    "#     print(end - start)\n",
    "    \n",
    "#     start = time.time()\n",
    "#     SVM = make_pipeline(StandardScaler(), SVR())\n",
    "#     SVM.fit(X_train, y_train)\n",
    "#     end = time.time()\n",
    "#     print(end - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    XGB = xgb.XGBRegressor(\n",
    "        booster='gbtree',    \n",
    "        n_estimators=1000,\n",
    "        objective='reg:squarederror',\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        colsample_bytree =0.6,\n",
    "        colsample_bylevel=0.6,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    XGB.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print(end - start)  \n",
    "\n",
    "    models = [\n",
    "#          LR, RF, SVM,\n",
    "              XGB]\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cfee81c-3f64-4687-993b-fcc9413a4f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate models on folds\n",
    "def CV(df, models, features, target):\n",
    "    cv = df.loc[df.index < '2017'].copy()\n",
    "    tss = TimeSeriesSplit(n_splits=4, test_size=6*24*30, gap=6*24)\n",
    "    \n",
    "    for model in models:\n",
    "        \n",
    "        fold = 0\n",
    "        preds = []\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, val_idx in tss.split(cv):\n",
    "\n",
    "            train = cv.iloc[train_idx]\n",
    "            test = cv.iloc[val_idx]\n",
    "            \n",
    "            X_train = train[features]\n",
    "            y_train = train[target]\n",
    "            \n",
    "            X_test = test[features]\n",
    "            y_test = test[target]\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "            preds.append(y_pred)\n",
    "            score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            scores.append(score)\n",
    "            \n",
    "        print(str(model)[:12])\n",
    "        print(f'Score across folds {np.mean(scores):0.2f}')\n",
    "        scores = [ '%.2f' % score for score in scores ]\n",
    "        print(f'Fold scores:{scores} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324c93d-226e-4ddc-8ee8-20218266001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating predictions\n",
    "def predict(X_test, df, models):\n",
    "    test_index = df.loc[df.index >= '2017'].index\n",
    "    test = pd.DataFrame(index=test_index)\n",
    "    \n",
    "    for model in models:\n",
    "        col = str(model)[:3]\n",
    "        start = time.time()\n",
    "        test[col] = model.predict(X_test)\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "    \n",
    "    df = df.merge(test, how='left', left_index=True, right_index=True).copy()\n",
    "#     df = convert(df)\n",
    "    df = df.astype('float')\n",
    "    df = df.round(1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a0890115-b5b4-49e6-bc73-ad21d350846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating metrics of ML models\n",
    "def metric(df, models, target):\n",
    "    ind = [str(model)[:3] for model in models]\n",
    "    cols = ['MAE', 'MAPE', 'MSE', 'RMSE', 'R2']\n",
    "    metrics = pd.DataFrame(index=ind, columns=cols)\n",
    "    df = df[df['XGB'].notna()]\n",
    "\n",
    "    for model in models:\n",
    "        col = str(model)[:3]\n",
    "        MSE = mean_squared_error(df[target], df[col])\n",
    "        RMSE = np.sqrt(mean_squared_error(df[target], df[col]))\n",
    "        MAE = mean_absolute_error(df[target], df[col])\n",
    "        MAPE = mean_absolute_percentage_error(df[target], df[col])\n",
    "        R2 = r2_score(df[target], df[col])\n",
    "        \n",
    "        metrics.loc[col] = MAE, MAPE, MSE, RMSE, R2\n",
    "        metrics = metrics.astype('float').round(4)\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "228c61b4-b82a-402a-988b-9e04dbdcd96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select params to tune using GridSearch\n",
    "def param_selection(model):\n",
    "    model = str(model)\n",
    "    if model.startswith('XGB'):\n",
    "        param_grid = {\n",
    "            'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
    "            'n_estimators': range(200, 1200, 200),\n",
    "            'learning_rate': [0.1, 0.05, 0.01],\n",
    "        }\n",
    "        \n",
    "    elif model.startswith('Hist'):\n",
    "        param_grid = {\n",
    "            'max_depth': [2, 3, 4, 5],\n",
    "            'learning_rate': [0.1, 0.01, 0.001],\n",
    "            'max_leaf_nodes': [3, 10, 30]}\n",
    "        \n",
    "    elif model.startswith('LGB'):\n",
    "        param_grid = {\n",
    "            'max_depth': [2, 3, 4, 5],\n",
    "            'learning_rate': [0.1, 0.01, 0.001],\n",
    "            'n_estimators': range(500, 1000, 100),\n",
    "            'num_leaves': range(50, 100, 10),\n",
    "            'min_data_in_leaf': range(100, 1100, 200)}\n",
    "        \n",
    "    elif model.startswith('Ada'):\n",
    "        param_grid = {\n",
    "            'loss': ['linear', 'square', 'exponential'],            \n",
    "            'learning_rate': [0.1, 0.01],\n",
    "#             'n_estimators': range(500, 1100, 100)}\n",
    "            'n_estimators': [50, 100, 200, 400]}\n",
    "        \n",
    "    elif model.startswith('RandomForest'):\n",
    "        param_grid = {\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'min_samples_leaf': [1, 2, 3]}  \n",
    "        \n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c734d-1f19-4b11-b9ae-0394b86d9022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def best_params(X_train, y_train, search):\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    grid = pd.DataFrame(search.cv_results_)\n",
    "\n",
    "    print(f\"The best parameters are {search.best_params_} with a score of {round(search.best_score_, 2)}\")\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ff86d-d20f-4804-a322-d864c3729214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate error and show timestamps of largest errors\n",
    "def errors(df, models, target):\n",
    "    for model in models:\n",
    "        col = str(model)[:3]\n",
    "        df[f'error_{col}'] = np.abs(df[df[col].notna()][target] - df[df[col].notna()][col])\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
