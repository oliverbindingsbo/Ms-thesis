{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac0438f2-b407-41c6-bde6-a14ef03e67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2727e816-0ca5-40f8-9051-d029ebcd5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].notna().all():\n",
    "            if col != 'Wind_Speed':\n",
    "                df[col] = df[col].astype(int)\n",
    "            else:\n",
    "                df[col] = round(df[col], 1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86da3787-eed1-4fda-98f8-d4373bffb751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_round(x, base):\n",
    "    return int(base * round(float(x)/base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22993313-a67c-4c02-82da-7bb1e90ec168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and clean the data. Requires csv files to be located in a folder called \"CSV\" in the directory of the notebook.\n",
    "def process_data():        \n",
    "    # Load the data\n",
    "    df_signals_2016 = pd.read_csv('CSV\\wind-farm-signals-2016.csv', sep=';')\n",
    "    df_signals_2017 = pd.read_csv('CSV\\wind-farm-signals-2017.csv', sep=';')\n",
    "    df_metmast_2016 = pd.read_csv('CSV\\wind-farm-metmast-2016.csv', sep=';')\n",
    "    df_metmast_2017 = pd.read_csv('CSV\\wind-farm-metmast-2017.csv', sep=';')\n",
    "    \n",
    "    # Merge dataframes\n",
    "    frames1 = [df_signals_2016, df_signals_2017]\n",
    "    df1 = pd.concat(frames1, ignore_index=True)\n",
    "    # Convert to datetime and remove time zone\n",
    "    df1['Timestamp'] = pd.to_datetime(df1['Timestamp']).dt.tz_localize(None)\n",
    "\n",
    "    frames2 = [df_metmast_2016, df_metmast_2017]\n",
    "    df2 = pd.concat(frames2, ignore_index=True)\n",
    "    df2['Timestamp'] = pd.to_datetime(df2['Timestamp']).dt.tz_localize(None)\n",
    "\n",
    "    # Join dataframes on index 'Timestamp'\n",
    "    df = df1.set_index('Timestamp').join(df2.set_index('Timestamp'))\n",
    "\n",
    "    # Select turbine\n",
    "    turbine = 'T07'\n",
    "    df = df.loc[df['Turbine_ID'] == turbine].reset_index()\n",
    "\n",
    "    # Extract only relevant columns\n",
    "    df = df[['Timestamp', 'Turbine_ID', 'Gen_Bear_Temp_Avg', 'Gen_Bear2_Temp_Avg',\n",
    "             'Gen_RPM_Avg', 'Nac_Temp_Avg', 'Amb_WindSpeed_Avg', 'Avg_Humidity', \n",
    "             'Gen_Phase1_Temp_Avg', 'Gen_Phase2_Temp_Avg', 'Gen_Phase3_Temp_Avg',\n",
    "             'Amb_Temp_Avg', 'Grd_Prod_Pwr_Avg',\n",
    "             'Amb_WindDir_Abs_Avg'\n",
    "            ]].copy()\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={'Timestamp': 'Date',\n",
    "                            'Gen_Bear_Temp_Avg': 'Gen_Bear_Temp',\n",
    "                            'Gen_RPM_Avg': 'Gen_RPM',\n",
    "                            'Gen_Bear2_Temp_Avg': 'Gen_Bear2_Temp',\n",
    "                            'Nac_Temp_Avg': 'Nac_Temp', \n",
    "                            'Amb_WindSpeed_Avg': 'Wind_Speed', \n",
    "                            'Avg_Humidity': 'Humidity',\n",
    "                            'Gen_Phase1_Temp_Avg': 'Gen_Phase1_Temp', \n",
    "                            'Gen_Phase2_Temp_Avg': 'Gen_Phase2_Temp',\n",
    "                            'Gen_Phase3_Temp_Avg': 'Gen_Phase3_Temp',\n",
    "                            'Grd_Prod_Pwr_Avg': 'Prod_Pwr',\n",
    "                            'Amb_WindDir_Abs_Avg': 'Wind_Dir',\n",
    "                            'Amb_Temp_Avg': 'Amb_Temp',\n",
    "                            'Grd_Prod_Pwr_Avg': 'Prod_Pwr'})\n",
    "\n",
    "    df['Gen_Phase_Temp'] = df[['Gen_Phase1_Temp', 'Gen_Phase2_Temp', 'Gen_Phase3_Temp']].mean(axis=1)\n",
    "    df = df.drop(columns=['Gen_Phase1_Temp', 'Gen_Phase2_Temp', 'Gen_Phase3_Temp'])\n",
    "\n",
    "    # Fill the missing Gen_Bear_Temp nan value with the the mean of the values diorectly next to it\n",
    "    df.Gen_Bear_Temp = df.Gen_Bear_Temp.fillna(48)\n",
    "    df = df.loc[df['Gen_Bear_Temp'] < 100]\n",
    "\n",
    "    # Combine duplicates by their mean\n",
    "    df = df.groupby(df['Date']).mean(numeric_only=True)\n",
    "    \n",
    "    #df = df.round(0)\n",
    "    df['Gen_RPM'] = df['Gen_RPM'].apply(lambda x: custom_round(x, base=5))\n",
    "    #df['Humidity'] = df['Humidity'].interpolate(method='spline', order=1, s=3)\n",
    "\n",
    "    df = convert(df)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06be73-714f-435f-b05e-2732243e6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "def train_models(FEATURES):\n",
    "    \n",
    "    # Split data\n",
    "#     FEATURES = ['Gen_RPM', 'Nac_Temp', 'Wind_Speed', 'Humidity', 'Gen_Phase_Temp', 'Amb_Temp']\n",
    "    TARGET = 'Gen_Bear_Temp'\n",
    "\n",
    "    df = pd.read_pickle(\"modeling.pkl\")\n",
    "    train = df.loc[df.index < '2017'].copy()\n",
    "    test = df.loc[df.index >= '2017'].copy()\n",
    "\n",
    "    X_train = train[FEATURES]\n",
    "    y_train = train[TARGET]\n",
    "\n",
    "    X_test = test[FEATURES]\n",
    "    y_test = test[TARGET]\n",
    "    \n",
    "    X_train_dropped = train.dropna()[FEATURES]\n",
    "    y_train_dropped = train.dropna()[TARGET]\n",
    "\n",
    "    X_test_dropped = test.dropna()[FEATURES]\n",
    "    y_test_dropped = test.dropna()[TARGET]\n",
    "    \n",
    "    # Train XGBoost ML model\n",
    "    XGB = xgb.XGBRegressor(booster='gbtree',    \n",
    "                           n_estimators=600,\n",
    "                           objective='reg:squarederror',\n",
    "                           max_depth=5,\n",
    "                           learning_rate=0.01,\n",
    "                           verbosity = 0,\n",
    "                           random_state=42)\n",
    "    XGB.fit(X_train, y_train,\n",
    "#            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "#            verbose=100\n",
    "           )\n",
    "    \n",
    "    # Train HGBR ML model\n",
    "    HGBR = HistGradientBoostingRegressor(max_depth=5,\n",
    "                                        learning_rate=0.1,\n",
    "                                        random_state=42)\n",
    "    HGBR.fit(X_train, y_train)\n",
    "    \n",
    "    # Train LightGBM ML model\n",
    "    LGB = lgb.LGBMRegressor(objective='regression',\n",
    "                             n_estimators=600,\n",
    "                             max_depth=3,\n",
    "                             learning_rate=0.1,\n",
    "                             num_leaves=50,\n",
    "#                             min_data_in_leaf=100,\n",
    "                             verbose=-1,\n",
    "                             random_state=42)\n",
    "    LGB.fit(X_train, y_train,\n",
    "#            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "            callbacks=[lgb.log_evaluation(period=0)])\n",
    "    \n",
    "    XGB_dropped = xgb.XGBRegressor(booster='gbtree',    \n",
    "                           n_estimators=600,\n",
    "                           objective='reg:squarederror',\n",
    "                           max_depth=5,\n",
    "                           learning_rate=0.01,\n",
    "                           verbosity = 0,\n",
    "                           random_state=42)\n",
    "    XGB_dropped.fit(X_train_dropped, y_train_dropped)\n",
    "\n",
    "    RF_dropped = RandomForestRegressor(n_estimators=100,\n",
    "                               max_depth=7,\n",
    "                               min_samples_leaf=2,\n",
    "                               n_jobs=-1,\n",
    "                               random_state=42)\n",
    "    RF_dropped.fit(X_train_dropped.values, y_train_dropped.values)\n",
    "    \n",
    "    linear_dropped = LinearRegression()\n",
    "    linear_dropped.fit(X_train_dropped, y_train_dropped)\n",
    "\n",
    "    return XGB, HGBR, LGB, XGB_dropped, RF_dropped, linear_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb376cbb-063f-447a-b8f5-57c298d42db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate models on folds plotted above\n",
    "def CV(models):\n",
    "    \n",
    "    df = pd.read_pickle(\"modeling.pkl\")\n",
    "    cv = df.loc[df.index < '2017'].copy()\n",
    "    tss = TimeSeriesSplit(n_splits=4, test_size=6*24*30, gap=6*24)\n",
    "    \n",
    "    FEATURES = ['Gen_RPM', 'Gen_Phase_Temp', 'Nac_Temp', 'Wind_Speed', 'Humidity', 'Amb_Temp']\n",
    "    TARGET = 'Gen_Bear_Temp'\n",
    "\n",
    "    for model in models:\n",
    "        \n",
    "        fold = 0\n",
    "        preds = []\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, val_idx in tss.split(cv):\n",
    "\n",
    "            train = cv.iloc[train_idx]\n",
    "            test = cv.iloc[val_idx]\n",
    "            \n",
    "            X_train = train[FEATURES]\n",
    "            y_train = train[TARGET]\n",
    "            \n",
    "            X_test = test[FEATURES]\n",
    "            y_test = test[TARGET]\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "            preds.append(y_pred)\n",
    "            score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            scores.append(score)\n",
    "            \n",
    "        print(str(model)[:3])\n",
    "        print(f'Score across folds {np.mean(scores):0.2f}')\n",
    "        scores = [ '%.2f' % score for score in scores ]\n",
    "        print(f'Fold scores:{scores} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa99b08-a2b6-4ffc-bf63-bf51a52aab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating predictions\n",
    "def predict(models, FEATURES):\n",
    "    df = pd.read_pickle(\"modeling.pkl\")\n",
    "    test = df.loc[df.index >= '2017'].copy()\n",
    "    test_dropped = test.dropna().copy()\n",
    "    \n",
    "    X_test = test[FEATURES]\n",
    "    X_test_dropped = X_test.dropna()\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        if i == 0:\n",
    "            test['XGB'] = model.predict(X_test)\n",
    "            test['XGB'] = test['XGB'].astype('float64')\n",
    "        if i == 1:\n",
    "            test['HGBR'] = model.predict(X_test)\n",
    "        if i == 2:\n",
    "            test['LGB'] = model.predict(X_test)\n",
    "        if i == 3:\n",
    "            test_dropped['XGB_dropped'] = model.predict(X_test_dropped)\n",
    "            test_dropped['XGB_dropped'] = test_dropped['XGB_dropped'].astype('float64')\n",
    "        if i == 4:\n",
    "            test_dropped['RF_dropped'] = model.predict(X_test_dropped.values)\n",
    "        if i == 5:\n",
    "            test_dropped['linear_dropped'] = model.predict(X_test_dropped)\n",
    "                \n",
    "    df = df.merge(test[['XGB']], how='left', left_index=True, right_index=True).copy()\n",
    "    df = df.merge(test[['HGBR']], how='left', left_index=True, right_index=True).copy()\n",
    "    df = df.merge(test[['LGB']], how='left', left_index=True, right_index=True).copy()\n",
    "    df = df.merge(test_dropped[['XGB_dropped']], how='left', left_index=True, right_index=True).copy()\n",
    "    df = df.merge(test_dropped[['RF_dropped']], how='left', left_index=True, right_index=True).copy()\n",
    "    df = df.merge(test_dropped[['linear_dropped']], how='left', left_index=True, right_index=True).copy()\n",
    "    df = convert(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0890115-b5b4-49e6-bc73-ad21d350846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating metrics of ML models\n",
    "def metric(df):\n",
    "    TARGET = 'Gen_Bear_Temp'\n",
    "    models = ['XGB', 'HGBR', 'LGB', 'XGB_dropped', 'RF_dropped', 'linear_dropped']\n",
    "    cols = ['MSE', 'RMSE', 'MAE', 'MAPE', 'R2']\n",
    "    metrics = pd.DataFrame(index=models, columns=cols)\n",
    "    df = df[df['XGB'].notna()]\n",
    "\n",
    "    for model in models:\n",
    "        if model in models[3:]:\n",
    "            df = df.dropna()\n",
    "        MSE = mean_squared_error(df[TARGET], df[model])\n",
    "        RMSE = np.sqrt(mean_squared_error(df[TARGET], df[model]))\n",
    "        MAE = mean_absolute_error(df[TARGET], df[model])\n",
    "        MAPE = mean_absolute_percentage_error(df[TARGET], df[model])\n",
    "        R2 = r2_score(df[TARGET], df[model])\n",
    "        \n",
    "        metrics.loc[model] = MSE, RMSE, MAE, MAPE, R2\n",
    "        metrics = metrics.astype('float').round(2)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c61b4-b82a-402a-988b-9e04dbdcd96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select params to tune using GridSearch\n",
    "def param_selection(model):\n",
    "    model = str(model)\n",
    "    if model.startswith('XGB'):\n",
    "        param_grid = {\n",
    "            'max_depth': [2, 3, 4, 5],\n",
    "            'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "            'n_estimators': range(500, 1100, 100),\n",
    "            'min_child_weight': [1, 10, 100]}\n",
    "        \n",
    "    if model.startswith('Hist'):\n",
    "        param_grid = {\n",
    "            'max_depth': [2, 3, 4, 5],\n",
    "            'learning_rate': [0.1, 0.01, 0.001],\n",
    "            'max_leaf_nodes': [3, 10, 30]}\n",
    "        \n",
    "    if model.startswith('LGB'):\n",
    "        param_grid = {\n",
    "            'max_depth': [2, 3, 4, 5],\n",
    "            'learning_rate': [0.1, 0.01, 0.001],\n",
    "            'n_estimators': range(500, 1000, 100),\n",
    "            'num_leaves': range(50, 100, 10),\n",
    "            'min_data_in_leaf': range(100, 1100, 200)}\n",
    "        \n",
    "    if model.startswith('RandomForest'):\n",
    "        param_grid = {\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'min_samples_leaf': [1, 2, 3]}  \n",
    "        \n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c734d-1f19-4b11-b9ae-0394b86d9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_params(search, model):\n",
    "    FEATURES = ['Gen_RPM', 'Nac_Temp', 'Wind_Speed', 'Humidity', 'Gen_Phase_Temp', 'Amb_Temp']\n",
    "    TARGET = 'Gen_Bear_Temp'\n",
    "\n",
    "    df = pd.read_pickle(\"modeling.pkl\")\n",
    "    train = df.loc[df.index < '2017'].copy()\n",
    "    test = df.loc[df.index >= '2017'].copy()\n",
    "\n",
    "    X_train = train[FEATURES]\n",
    "    y_train = train[TARGET]\n",
    "    \n",
    "    X_train_dropped = train.dropna()[FEATURES]\n",
    "    y_train_dropped = train.dropna()[TARGET]\n",
    "    \n",
    "    model = str(model)\n",
    "    \n",
    "    if model.startswith('RandomForest'):\n",
    "        search.fit(X_train_dropped, y_train_dropped)\n",
    "        \n",
    "    else:\n",
    "        search.fit(X_train, y_train)\n",
    "\n",
    "    grid = pd.DataFrame(search.cv_results_)\n",
    "\n",
    "    print(f\"The best parameters are {search.best_params_} with a score of {round(search.best_score_, 2)}\")\n",
    "    grid\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ff86d-d20f-4804-a322-d864c3729214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error and show timestamps of largest errors\n",
    "def errors(df):\n",
    "    TARGET = 'Gen_Bear_Temp'\n",
    "    df['error_XGB'] = np.abs(df[df['XGB'].notna()][TARGET] - df[df['XGB'].notna()]['XGB'])\n",
    "    df['error_HGBR'] = np.abs(df[df['HGBR'].notna()][TARGET] - df[df['HGBR'].notna()]['HGBR'])\n",
    "    df['error_LGB'] = np.abs(df[df['LGB'].notna()][TARGET] - df[df['LGB'].notna()]['LGB'])\n",
    "    df['error_XGB_dropped'] = np.abs(df[df['XGB_dropped'].notna()][TARGET] - df[df['XGB_dropped'].notna()]['XGB_dropped'])\n",
    "    df['error_RF_dropped'] = np.abs(df[df['RF_dropped'].notna()][TARGET] - df[df['RF_dropped'].notna()]['RF_dropped'])\n",
    "    df['error_linear_dropped'] = np.abs(df[df['linear_dropped'].notna()][TARGET] - df[df['linear_dropped'].notna()]['linear_dropped'])\n",
    "#     test = test.merge(test_dropped[['error_XGB_dropped']], how='left', left_index=True, right_index=True).copy()\n",
    "#     test = test.merge(test_dropped[['error_RF_dropped']], how='left', left_index=True, right_index=True).copy()\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
